{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hard-WSDClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNo7EKAJCYq6HNhhHM+x2cF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/summer1278/UoM-NLP-student-homework/blob/main/Hard_WSDClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiETu-lYoDQq",
        "outputId": "07a5262f-e566-4619-d73e-13bc34652a38"
      },
      "source": [
        "# mount my googl drive for loading files directly from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzRS_TjtgwYV"
      },
      "source": [
        "# libs\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRcAGFtApibg"
      },
      "source": [
        "sentences = []\n",
        "sense_labels = []\n",
        "label_dict = {'HARD1':1, 'HARD2':2, 'HARD3':3}\n",
        "\n",
        "with open('/content/drive/My Drive/data/hard.cor') as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    # find sentence in the corpus\n",
        "    if '<s>' in line:\n",
        "      # remove <s></s> sentence tag and quotes\n",
        "      text = line.replace('<s>','').replace('</s>','').replace(\"``\",\"\").replace(\"''\",\"\")\n",
        "      # find the HARD's label and convert it to a numeric value for easy computation\n",
        "      if 'tag' and 'HARD' in text:\n",
        "        for key in label_dict.keys():\n",
        "          if key in text:\n",
        "            label = label_dict[key]\n",
        "      # remove all tags including sense tags\n",
        "      text = re.sub(re.compile('<.*?>'), '', text)\n",
        "      # lower the case of words and remove line breaks \\n\n",
        "      text = text.lower().replace('\\n', ' ').strip()\n",
        "      # remove other nonsense symbols and spaces, and keep alphabets\n",
        "      text = re.sub(r'&[a-z]+;', '', text)\n",
        "      text = re.sub(r'\\s+', ' ', text)\n",
        "      # append them to the collector\n",
        "      sentences.append(text)\n",
        "      sense_labels.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea5mURsglHVs",
        "outputId": "9fac18fa-8380-445d-f3cc-7dd5a40c4dfd"
      },
      "source": [
        "# Bag-of-Word feature with sklearn\n",
        "vectorizer = CountVectorizer(analyzer='word',stop_words='english')\n",
        "X = vectorizer.fit_transform(sentences).toarray()\n",
        "# vocabulary length\n",
        "print(len(vectorizer.get_feature_names()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWkX_ZIqmdqS"
      },
      "source": [
        "y = sense_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7I0l41bm4MK"
      },
      "source": [
        "# now we split the train and test set for classification\n",
        "# we make 70% for train and 30% for test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea5bJSD4oPbC",
        "outputId": "e3e1a2b2-6e35-4700-8fd2-157c6f73a417"
      },
      "source": [
        "# we fit the model using Gaussian NB\n",
        "clf = GaussianNB()\n",
        "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6915384615384615"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-QCL--ColBi",
        "outputId": "3158dd75-2a3e-4435-eb98-adbaae9ba7e3"
      },
      "source": [
        "# SVM is sensitive to the scale of feature values, we can use a standard scaler\n",
        "# you can use pipeline to join the scaler and SVM classifier\n",
        "clf = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000, tol=1e-3))\n",
        "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8153846153846154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlWJorPkp55q",
        "outputId": "4c0db9bf-ebfa-4fa2-edd7-83ea09a657b3"
      },
      "source": [
        "# another feature, TF-IDF \n",
        "# let's take most frequent 3000 features\n",
        "vectorizer = TfidfVectorizer(analyzer='word',stop_words='english',max_features=3000)\n",
        "X = vectorizer.fit_transform(sentences).toarray()\n",
        "# feature vector length\n",
        "print(len(vectorizer.get_feature_names()))\n",
        "# train and test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf5V54o_rTXa",
        "outputId": "98d6a6d3-5d32-4e80-f309-bb375a226cba"
      },
      "source": [
        "# let's we try the same classifiers\n",
        "clf = GaussianNB()\n",
        "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6676923076923077"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKCOGkWrrd-x",
        "outputId": "649ae5de-424d-4f9f-dbf2-f5219e9d8388"
      },
      "source": [
        "clf = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000, tol=1e-3))\n",
        "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8176923076923077"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}